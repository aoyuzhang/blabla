\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amscd, amsthm, amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[procnames]{listings}
\usepackage[english]{babel}
\usepackage{color}
\usepackage{tcolorbox}
\usepackage[document]{ragged2e}
\usepackage{ragged2e}
\tcbuselibrary{minted,breakable,xparse,skins}

\definecolor{bg}{gray}{0.95}
\DeclareTCBListing{mintedbox}{O{}m!O{}}{%
  breakable=true,
  listing engine=minted,
  listing only,
  minted language=#2,
  minted style=default,
  minted options={%
    linenos,
    gobble=0,
    breaklines=true,
    breakafter=,,
    fontsize=\small,
    numbersep=8pt,
    #1},
  boxsep=0pt,
  left skip=0pt,
  right skip=0pt,
  left=25pt,
  right=0pt,
  top=3pt,
  bottom=3pt,
  arc=5pt,
  leftrule=0pt,
  rightrule=0pt,
  bottomrule=2pt,
  toprule=2pt,
  colback=bg,
  colframe=orange!70,
  enhanced,
  overlay={%
    \begin{tcbclipinterior}
    \fill[orange!20!white] (frame.south west) rectangle ([xshift=20pt]frame.north west);
    \end{tcbclipinterior}},
  #3}
  
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

% \theoremstyle{remark}
% \newtheorem*{remark}{Remark}

% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{lemma}[theorem]{Lemma}

\oddsidemargin 0pt
\evensidemargin 0pt
\marginparwidth 40pt
\marginparsep 10pt
\topmargin -20pt
\headsep 10pt
\textheight 8.7in
\textwidth 6.65in
\linespread{1.2}

\title{Agriculture-food and no agriculture food industry classification of the collected web pages of the federally incorporated companies}

\author{Hao Yu Zhang  \\
	University Of Montreal  \\
	}
	
% \author{Hao Yu Zhang \\University of Montreal\\Supervised by Professor Sandra Schillo and Professor Nie Jian Yun}

\date{\today}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}

\newcommand{\rr}{\mathbb{R}}

\newcommand{\al}{\alpha}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\aff}{aff}

\begin{document}

\maketitle


\center{Supervised by professor Nie Jian Yun(University of Montreal) \\ \\ \\ \\ \\ \\ \\ \\
Worked under professor Sandra Schillo(University of Ottawa)}


% General Information (cover page):
% Student name, number
% Title of internship project
% Name, contact information and definition of the host organization (company, public organization, association, etc.)
% Name and contact information of the internship supervisor
% Introduction
% Context / Issue, information about the organization, field of activity, number of employees, location, years of existence, etc., project within the organization that encompassed the internship (if applicable): objectives, duration, participants, etc.
% Objectives of the project pursued during the internship and work required
% Timeline
% Work environment
% Presentation of the designed systems and their integration into the work environment
% Technical problems encountered and their resolution
% Critical evaluation of the contribution of the internship to the student's training: main outcomes of the internship, intermediate choices that could have been done differently (if applicable).
\begin{flushleft}
\newpage
\tableofcontents
\newpage

\section{Introduction}\label{section-introduction}
% We are interested in creating a program that can crawl over the internet to find and classify company websites into correct company web page and not correct company web page for a list of one million federally incorporated companies. Further, if the website belongs to a company, We want to follow the NAICS industry standard to classify each company website into the correct industry. We are interested in classifying the company into agriculture related company and no agriculture related company. We made use of current machine learning and deep learning techniques and applied them to website classification. our method is scientific in that it uses the NAICS industry definition in order to determine the industry of a web page. We will show that our method perform well in classifying the websites. 



% An internet crawler is a spider that crawls over the internet from one web page to another via its hyperlink references. The internet is like a giant abstract web.  When We click on a link in a web page to jump to another web page, We can think of the two web pages as connected by an invisible wire. If We get to page B by clicking on a link on page A, that link is directed towards page B. Nevertheless, if We forget about directions on the link between pages, then all of the web pages in the world and the links between them look like a giant network and a giant web.  So then We are like spiders moving from one web page to another via their connecting wire.  If We want to find a specific web page and start from some random web page, how will We get to that specific web page? That is how a program that navigates through the internet and finds the web page We are looking for is helpful.



There are about one million federally incorporated companies in Canada, for which basic corporate information, such as name and address is available publicly. However, there is little additional information, in particular information pertaining to the company’s activities or industry sector is not publicly available. and we don’t have much information on what they do. This lack of industry information severely limits the usability of public information on companies, as many analyses depend on sector-specific knowledge. 


Thus, the goal of this project is to develop a method to identify industry activities of companies, using only the public information of companies as a starting point, augmenting the data by adding corporate URLs and downloading the text from the corporate web sites, and then classifying companies by their activities, following the standard North American Industry Classification System (NAICS). To develop this method, this project focuses on the agri-food sector, refining the model to recognize related activities, and linking with a larger project exploring the sectors’ companies and activities. 

 



% The two technologies is essential towards information retrieval and information  understanding. We will be focusing on develop specific tools in those area to solve our problem. The report is organized as follow.  In sections \ref{Information about the organization} We introduce our company and a bit of its history.  In sections \ref{section-Objectives of the project pursued during the internship and work required} We state our objective of the internship.  In Section \ref{section-applications} We describe some applications of Tverberg's theorem.  Finally, in Section \ref{section-spaces} We present Tverberg-type results where the settings have changed dramatically, such as Tverberg for convexity spaces or quantitative versions.  In that last section, We focus mostly on results which are related to geometry.

% \section{Information about the organization}
% We did our internship at the Telfer School of Management of Ottawa University in Canada. The school was established in 1969, so it is a relatively young school in Canada. Nevertheless, its reputation has been quickly climbing over the decades and currently has a good reputation globally and in Canada. It is consistently ranked in the top 250 business schools globally and Canada's top six business schools.  It received once the largest donation to a business school from alumni in Canadian history and is one of only three business schools in North America to have triple accreditations(By the AACSB, the Association of MBAs, and the EFMD).   So we were fortunate to have been allowed to work there.  This way, we are supervised by kind, knowledgeable and helpful professors and exceptional and friendly students.  It is not to say that tech companies do not have brilliant people, but We must admit that having professors as supervisors for an internship is a big plus. 

% See figure ~\ref{fig:22}. 
% \begin{figure}[h!]
%   \centerline{\includegraphics[scale=0.6]{telfer.JPG}}
%   \caption{The Telfer school of management}
%   \label{fig:22}
% \end{figure}




\section{Objectives of the project pursued during the internship and work required}

% We are doing an internship as part of the graduation requirement for our computer science master's diploma. In our internship. In more detail, 

We have a list of registered company names and addresses, and we want to find the company's web page by navigating through the internet starting from Google.com. If we were to do it manually, we would search the company name in Google, and we would examine each of the returned results one by one to see if the returned web page could be the company's web page. However, this is a very time-consuming task. Therefore, we want to do this quickly and accurately to have a list of company web pages and their content at the end of the day. The content of the web pages will help us determine the company's industry, and we are interested in finding agri-food related companies. So we will build another program that will classify the industry company based on the information on their corresponding web pages and use it to classify the company web pages that we found.
So we can break our task into two parts, data augmentation and classification.

\begin{enumerate}
    \item Data augmentation, we have a list of federally incorporated company names, and we need to write a web crawler to collect the web pages and contact information of those companies. We need first to do a Google search and analyze the top results to see if the website can correspond to the company for this task. We will use heuristics such as the similarity between company name and URL and the company's contact information on the web page to determine whether the web page corresponds to that of the company. We will store all of the collected data on a MongoDB database.
    
    \item Classification:  We will train a classifier from a training data set. This classifier will help us determine if the web page of a company is agri-food related or not. The training data are either given or collected from the internet with a crawler program that we develop. We will compare several classification algorithms for their accuracy ranging from traditional to deep learning methods. We will then use our best model to classify the web pages we found in our task's first part. 
\end{enumerate}

% So our report is then organized as follows: first, We will introduce our company and our work environment. We will explain how our project fits into perhaps the bigger picture of the company's goal. We will explain how We managed our time to progress towards the goal of our project. We will make precise the tasks of our project.  Then We will explain our project, including the technical and no technical challenges that We faced, the technical tools We used, the solution We came up with, and the results We got.  The essential technical background will be in the main body of the report, and We will put any additional technical background that We judge necessary into the appendix section of the report.  We will also explain our programming codes in a subsection to make a replication study possible. 


\section{Timeline}
We started working in May 2021. We devoted the first month to understanding the basics of web scrapping and MongoDB database by learning the necessary python libraries devoted to web scrapping and connecting to a MongoDB database. We had prior experience with writing HTML code, so We were comfortable with understanding how HTML work, and that helped us quickly learn how to write a small scrapping program. We devoted the next two months to making and improving our scrapping program to scrape the web page's content of the list of federally incorporated companies. Then for the later three months, we focused on making, testing and comparing classification models to distinguish the agri-food companies from the no-agri-food companies. Then near the end of our internship, we used our best model to classify the industries of the web pages that our scrapping program scraped. The figure ~\ref{fig:16} shows our timeline.
\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.6]{cap37.jpg}}
  \caption{Time line of the internship}
  \label{fig:16}
\end{figure}



\section{Work environment(Data and Resources)}
% Although the University of Ottawa resides in Ottawa, We did the internship remotely and participated in the meetings via online conference. our work is part of a project funded by MITACS that involves the business data of federally and provincially incorporated companies. During the internship, We worked alone and participated in the weekly meeting to update on the project. For the six-month duration of our internship, We focused on two things: web search and machines-learning-based classification. We document our progress frequently and focus on improving our results. 

\subsection{Data sets}
We now present the datasets we collected and how we used them in the project. 
\begin{itemize}
    \item \textbf{The federally incorporated companies dataset}:  The primary dataset of our project is the list of federally incorporated companies that we can download in the statistics of Canada website accessible in \href{https://open.canada.ca/data/dataset/0032ce54-c5dd-4b66-99a0-320a7b5e99f2}{\textcolor{blue}{this URL address}}. We can go on to that URL and click on the download button to download the list of the company as shown in figure ~\ref{fig:fed_company_stat_can_webpage}. 
\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.35]{cap28.jpg}}
  \caption{Download the federally incorporated company list}
  \label{fig:fed_company_stat_can_webpage}
\end{figure}

The downloaded file is in XML format as We can see in the figure ~\ref{fig:fed_incorporated_company_xml}. We need to extract the information of the company from the XML code. We extracted the current address, the previous addresses, the company id, the corporation id, the current name and the previous names of the company. We saved all this information in a JSON file that the scrapping program will use. 
\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.35]{cap30.jpg}}
  \caption{The downloaded list of federal incorporated companies}
  \label{fig:fed_incorporated_company_xml}
\end{figure}

We devotedly wrote and tested a scrapping program to search for company URLs for the 1 million federally incorporated companies for two months. During this time, We tried several methods to help us determine if a web page is that of the company. This first version of the scrapping program only considered a company's current name and address when trying to find its web page. It would only look for the existence of the company address within the contact page, the home page and the about page of the website to determine if the web page is that of the company. We found that the program is not perfect due to implementation detail. Sometimes, the program fails to scrap the visible content of the web page, or the program scrapped the wrong encoding of the text, and the result is that We scrapped incomprehensible texts.
Further, when the Internet disconnects, We have a hard time keeping track of the progress of the scrapping and often need to restart searching the web page from the beginning of the company list. Hence, We rewrote the scrapping program by the end of the third month. The new program used past company names and addresses and would check web pages from all hyperlinks of the home page to check for one of the company addresses to determine if the web page belongs to the company. The new version is robust to sudden interruption of execution. When the internet disconnect, We need to rerun the program, and it will restart from where it left off. The second scrapping program is continuously running to gather information on the 1 million federally incorporated companies until the end of the internship. 
 
\item \textbf{CLEARBIT dataset}: Our first program to find company web pages used a freely available online API service from CLEARBIT.com that provided a list of companies with their web page and logo picture and whose name contains the query to the API. We scraped a set of companies with their web pages with this API service. We stored the scrapped data in the school MongoDB server, and this data provided us with a list of web pages for us to understand how to write our scrapping program to scrap the content of those web pages and store the scrapped information on the MongoDB server. We then wrote a scrapping program to scrap the content of a subset of the Clearbit webpages and stored them in the school MongoDB database. 

\item \textbf{SICCODE dataset}: The Clearbit dataset did not have the companies' industry labels, but we needed some labelled training data for industry classification. We found a company database accessible via URL SICCODE.com. In SICCODE.com, the company list comes with an industry label using the North American Industry Classification System(NAICS). However, the companies' web page was missing, so we wrote a scrapping program to search for the company web page in Google. We scraped about 10000 company web pages from SICODE.com, whose NAICS label is four digits and stored them in the school MongoDB database.  


\item \textbf{LODE dataset(with NAICS label)}
We were fortunate to have been provided with a list of company web pages labelled with the 6 digits NAICS code. This dataset is in the LODE Github repository accessible in this \href{https://github.com/CSBP-CPSE/LODE-ECDO/tree/master/sources/Business/on}{URL}. All we needed to do to make this a good training set for our classification program was to scrap the content of the provided web pages. Simply scrapping the web page's content is much simpler than if we were also to find the companies' web page. Hence, we build this dataset in a concise amount of time. 

\end{itemize}

The companies from SICCODE.com and the LODE Github repository are labelled data, and they are the training dataset for our supervised classification models. Also, they have helped us perform a performance check of the industry classification algorithm, both supervised and unsupervised, that we have created. The LODE GitHub data has only about 100 agriculture-related company data.    Hence, most of our agriculture-related company web pages come from the SICCODE dataset. We took a screenshot of the collected SICCODE and LODE Github datasets. 
see figure ~\ref{fig:25} and figure  ~\ref{fig:26}. 
\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.6]{cap49.JPG}}
  \caption{The scrapped SICCODE.com company web pages}
  \label{fig:25}
\end{figure} 

\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.6]{cap50.JPG}}
  \caption{The scrapped GitHub company data}
  \label{fig:26}
\end{figure} 


\subsection{Resources / Technology Stack}
We used the Python programming language to develop the scrapping programs and train the classification models for our project. For the scrapping program, we used the Beautifulsoup, Requests, Urllib libraries to write the codes to get the content of the web pages. We used the MongoDB database to store all the scrapped data. We used the Pymongo library to connect our program to the MongoDB server. We used the Sklearn library to create the classical natural language models such as SVM or random forest. We used the Huggingface library to make more state of the arts pre-trained transformer-based natural language models such as BERT or XLNET and fine-tune the models using the scrapped web pages. It was easy to find tutorials on Google about using Python or MongoDB or Hugging face Libraries. We can often find prewritten codes on how to perform a specific task and adjust those codes for our particular goal. 

% \section{Presentation of the designed systems and their integration into the work environment, technical problems encountered and their resolution}




\section{Data Acquisition}

\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
 
\lstset{language=Python, 
        basicstyle=\ttfamily\small, 
        keywordstyle=\color{keywords},
        commentstyle=\color{comments},
        stringstyle=\color{red},
        showstringspaces=false,
        identifierstyle=\color{green},
        procnamekeys={def,class}}
 

 
 The goal of the scrapping program is to find the web page of a company given its name and scrap information on the web page, and store the scrapped information in a database to be used for industry classification.\\\\
We now explain the scrapping programs that we used to search on Google for the company web page.  The program has been changed over and over by noticing the flaw of the program and fixing them. There are freely available APIs such as Clearbit.com that provide the URL of a company given its name. However, the APWe service is for commercial purposes and does not always contain the information of the companies that We are interested in. We need to develop a scraping program that will find the company web page for us. 

What We do is to first do a Google search for the name of the company. The search is done using the Google python library that is available to use by installing it via
\begin{lstlisting}
pip install google 
\end{lstlisting}

An alternative way to perform a Google search is to use the Google search APWe by setting up a Google account. Both options are valid, but We found that using Google API, sometimes, if We make too many requests using Google search API, the service will be no longer available, and We may or may not have the service working even if We wait for it for a while. Hence, for our purpose, We found that the google library can do the job well, and We used it in our final scrapping program. An example of Google search for using the installed Google library can be seen in figure ~\ref{fig:google_search}:
\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.6]{google_seach.JPG}}
  \caption{Performing a Google search in python for the term  agriculture company and get the first 6 results}
  \label{fig:google_search}
\end{figure}

With Google search available, We then search the name of the company with Google search and then the first $6$ results of the Google search will be analyzed.  We examine the returned URLs and keep track of the frequently returned web URLs. These commonly returned web pages are other form of search engine that has a database of companies. These web pages are not the web page of the search company but contains the information of the search company. We keep track of these kind of web pages and made a list of them. These will be the list of bad URLs. 
We can refer to figure ~\ref{fig:8} for the flow of the scrapping program: 

\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.6]{cap29.jpg}}
  \caption{The flow of the scrapping program}
  \label{fig:8}
\end{figure}

In the figure, each of the six outputs URLs of the Google search is first compared with URLs in the list of bad URLs. If the URL is in the list of bad URLs, then it will not be considered further.  The list of bad URLs is continuously built and gradually increases in size. They make the recognition of the correct company web page more efficient because the complicated and time-consuming checks that need to be done can be reduced by checking if the returned Google search web page is in the list or not. 

A problem that We encountered is that requesting a web page using a program may not always succeed, and sometimes, the program halt and wait for a response from the web page hosting service. our internet connection is weak, and these two problems caused us much time scraping the web pages. The solution to this problem is to increase the number of programs to scrap the web page. For this part, We got four computers to scrap the web pages and several programs running simultaneously. We made several Google accounts to use the Google Colab to help us, and each program is responsible for part of the list of federally incorporated companies. 

To check whether a web page is that of the company, our first idea is to check if the web page's contact section contains the company's address; if so, We are very confident that the web page is that of the company. However, this method only finds a tiny percent of company web pages, and We need to have additional ways to find more companies. our first method to see if the web page is that of the company without the use of the address check is to compute a similarity scoring between the name of the company and the URL of the web page. We consider several factors such as if the abbreviation of the company name is in the URL or the percentage of words match, or the fuzzy matching method such as the modified version of edit distance between company name and URL of the web page.  Then We gave a threshold for when a web page can be considered the web page of the company.  By then, We moved on to using a more sophisticated method such as using a transformer-based model to do a sentence similarity task between company name and URL. However, because We do not have a lot of labelled data and the company owner could give arbitrary domain names for his company, this method is eventually not used. Instead, We focused on improving the correctness and performance of the scrapping program so that it is more time-efficient and robust to internet disconnection..  

 There are several checks for if the web page belongs to the company. In our final version of the scrapping program, We are making sure that the version of the company name that is stripped of the "ltee", "inc.","ltd.","limited","inc","corp","ltd","corporation" is also considered. We will simply call the list ["ltee", "inc.","ltd.","limited","inc","corp","ltd","corporation"] the trailing part of a company for short. We list the checks:
\begin{enumerate}
\item We check if the company's name appears in the title of the web page. This check is done by seeing if the lowercased company name and the version where We stripped its trailing part is a sub-string of the title string of the web home page.  
    \item We check if the company's name in the lower case appears as a substring of the domain name of the web page in the lower case. Moreover, We do the same for the company name stripped of its trailing part as before. 
    \item We check if the company's current address is in any of the sub-pages of the web page. This check works by extracting the ZIP code and the numerical part of the address using already available python regular expressing packages. Then We check if the ZIP code in its different format and the numerical part of the address both appear as a substring of the content string of each of the sub-pages of the web page. We only check for the linked sub-pages of the website's main page and not the distance of two or more hyperlinks from the main page.
    \item We check any of the previous company addresses appears in any of the sub-pages of the web page. It is the same procedure for checking if the current company address is on any of the content of the website's sub-pages. 
    \item  Here, the company name is first split into words, and then the first letter of each word is grouped to form the abbreviation of the company name. We check if the abbreviation of the company name matches the domain name of the URL perfectly. Then We check if the abbreviation string matches the domain name of the URL perfectly. 
    \item We check if the company name and the company name without the trailing part are a complete match with the domain name of the URL. We check if the company name, after removing all delimiter characters, would match the domain name of the URL perfectly.  
\end{enumerate}
 We are pretty sure that if the current company address or the previous company addresses appear in some sub-page of the home page of the web page or that there is a perfect match between the company name and the URL or there is a perfect match between the abbreviation of the company name and the URL, then that web page is that of the company. For the case of when the company name appears in the title of the web page or that the company name appears in the URL of the web page, We will keep them as well, but We will not be $100$ percent sure that they are the web page of the company. 
 
 Another main problem that We have to deal with is deciding what information from the web page We need to scrap. From previously done work in this area, We found that the title, description and keywords of the web page are very useful in helping determine the nature of the web page. With the guidance from our supervisors, We decided to scrap several sub-pages such as the about page and the product page to help us with our industry classification task. The following are the information that We noted for each web page:
 \begin{enumerate}
     \item The current company name
     \item The previous company names
     \item The URL of the company web page
     \item Whether the company is active or not
     \item The current address of the company 
     \item The previous addresses of the company 
     \item The corporation id of the company
     \item The business number of the company
     \item The title of the main page
     \item The description of the main page
     \item The keywords of the main page
     \item The home page of the website
     \item The about page of the website
     \item The contact page of the website
     \item The product page of the website
     \item The service page of the website
     \item The directory page of the website
     \item A set of images URLS for the home page 
     \item Whether the current address of the company is in the web page.
     \item Whether the previous addresses of the company is in the web page. 
     \item Whether the searched name is in the URL of the company web page.
     \item Whether the searched name is in the title of the home page. 
     \item whether the abbreviation of the search name is in the domain name of the web page. 
     \item Whether the search name is in the domain name of the web page. 
 \end{enumerate} 
 We find those sub-pages by examining all of the home page's $<a>$ tags. We will check the sub-page if the $<a>$ tag content contains the appropriate keywords. For example, if the content of an $<a>$ tag is "contact us" or "contact" or"reach us," then We will think that the hyperlink for that $<a>$ tag will lead us to the contact page of the website and We will scrap the content of that web page and make it the contact page of the website. We put two figures showing our scrapped result and we can see in the figures that our scrapping program is successful. See ~\ref{fig:scraped_federally_incorporated_company_detail} and figure ~\ref{fig:scraped_federally_incorporated_company_list}.
 
 \textcolor{red}{We selected 100 random samples and examine their correctness. The correctness of the web pages for a given company is 93\%.} 
 
\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.8]{cap43.jpg}}
  \caption{The scrapped federally incorporated company web pages}
  \label{fig:scraped_federally_incorporated_company_detail}
\end{figure} 

\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.45]{scrap_result_list.PNG}}
  \caption{The list of scrapped federally incorporated company web pages}
  \label{fig:scraped_federally_incorporated_company_list}
\end{figure} 


\section{agriculture-food no agriculture food  classification}
\subsection{classification models}
We have used several approaches for the industry classification part of the project. We will explain each of the methods involved and compare the results. The tests are done on a set of NAICS labelled data that We have about 3159 web pages labelled with the six digits NAICS code . 
We experimented with those data, including about 150 agriculture web pages that We scrapped from SICCODE.com. The description of the testing data can be seen in the figure:


The classification methods are as follow:

\begin{enumerate}
\item  We first present an unsupervised learning method. This method primarily uses keywords and short sentences(usually two or three words) for classification. See figure ~\ref{fig:keywords_classification_flow} for illustration.  For the classification task, We need a list of definitions for each classification class. These definitions are the NAICS definitions and NAICS examples from the statistics of Canada web page. Hence making the model understand the NAICS definition and NAICS examples is of great importance when doing classification. 

We want our program first to classify the web page using two digits NAICS code. This way, We would be able to say whether the company web page is agriculture-related or not by seeing if the predicted label is $11$ or not. The NAICS $11$ code is the NAICS code for the agriculture industry. Then We move on to do a more refined classification to give a more refined label for the company web page. 

For the part where We determine the two digits NAICS code label of the web page, We use a list of keywords and short sentences for the industry. The basis of the keywords is the NAICS examples and the selected keywords extracted using TF-IDF score on the NAICS definition and examples. The set of keywords and short sentences are further enlarged by manually selecting texts from the analysis of some of the labelled webpages. The built keywords and short sentences are in a python dictionary where the key is the $2$ digits NAICS code, and the value is a list of keywords or short sentences associated with that NAICS code. We used these keywords and short sentences by either counting their occurrences in the web page content or performing a semantic comparison with the web page content using a language model. 

More specifically, We check if there is a good amount of text in the description of the webpage, the title of the web page. If there is a sufficient amount of text, usually more than a word in them, We will see how many of those words are in the set of keywords for our 2-digits NAICS code. For a keyword of length 1, We match it against a list of words after separating the title and description and URL and company name into a list of single words. Instead, We will check for their inclusion in the web page's description and title for short key sentences. 

Then We pick the 2-digits NAICS code with the most matched keywords as the 2-digits NAICS code for that company web page. To determine more digits of the NAICS code, We first encode each sentence in the NAICS description and examples into a 768-dimensional vector using a pre-trained transformer model called paraphrase-mpnet-v2 from the python's Hugging face library that provides many states of the art transformer-based language models. Then We also encode the company's name, the web page description, and the web page's title into a 768-dimensional vector. We then do a cosine similarity computation between the encoded web page title and description with all the encoded NAICS definitions and examples. We also compute the similarity between the web page's title with the company name and the web page's description with the company's name. Then for each title and description, We multiply the result of their similarity score with the NAICS definition with their similarity score with the company's name. Hence We would get a list of NAICS definitions and examples and their similarity scoring with both the title and description of the web page.  We look at the top $10$ scored sentences and pick the most frequent NAICS code associated with the ten sentences as the NAICS code for that web page. We illustrate the last step in figure ~\ref{fig:29}. 
    
     
\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.8]{cap44.jpg}}
  \caption{The keyword method for industry classification}
  \label{fig:keywords_classification_flow}
\end{figure} 

\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.6]{cap51.JPG}}
  \caption{The sentence comparison part of keyword method}
  \label{fig:29}
\end{figure} 
    
    
    % We will illustrate with an example. Say We have a scrapped web page with the following information:
    % $$\begin{cases}
    % \end{cases}$$


    % \item This method is devided into 4 stages:
    % \begin{enumerate}
    %     \item Generating industry vectors: Extract sub-industry information for all industries to be classified, after removing the stop words from the  words that describe the industry, using an online word classification tool, We can group the words to their appropriate industry as the external knowledge when it comes to classify a company. Convert the classified words to 300 dimensional vectors. Here We will use $ind(i,j,z)$ as the vector embedding of the $z^{th}$ word of the $j^{th}$ sub-industry of the $i^{th}$ industry. See figure ~\ref{fig:classify_step1}
    %     \begin{figure}[h!]
    %       \centerline{\includegraphics[scale=0.6]{classify_step1.JPG}}
    %       \caption{Step 1, create word embedding matrix}
    %       \label{fig:classify_step1}
    %     \end{figure} 


    %     \item (2)Make a idf word weighting: the frequency of appearance of a word can be used to automatically separate important words from no important words and help improve accuracy of industry classifier. More specifically, consider a word $w$ in the set of words in the content of a company web page after the removal of stop words. The corresponding idf value of that word is $$ idf(w) = \log(\frac{N}{count(w)+1})$$ 
    %     Here $N$ is the total number of words in the text and $count(w)$ is the frequency of the word $w$ in the text. We then normalize the value using the formula $$idf_{norm}(w)=\frac{idf(w)-idf_{min}}{idf_{max}-idf_{min}}$$.
    %     Here $idf_{min}/idf_{max}$ is the smallest/largest idf value for the words in the text. 
        
    %     \item (3) prepossessing company information: Find keyword in the company descriptions using textrank algorithm and convert each word into a 300 dimensional vector. Denote $v(i)$ as the vector embedding of the $i^{th}$ word in the text. The text rank algorithm is based on page rank algorithm. 
        
    %     We can understand it as a person who start on a web page and travel randomly to another web page via one of the current web page's hyperlink and repeat the process. The probability that one visit a web page is then the rank of that web page. We now define this notion formally and  see some results. 
       
        
        
    %     The page rank algorithm calculate a page rank for a web page $p$ by the formula:
    %     $$ PR(P_i) = \frac{1-d}{N} + d \sum_{p_j \in M(p_j)} \frac{PR(p_j)}{L(p_j)}$$
    %     Where  $M(p_i)$ is the set of web pages that has a hyperlink to web page $p_i$, $L(p_j)$ is the web page $p_j$'s number of connections to other web pages and $d$ is a constant called damping factor that is trying to mimic the probability that the probability that web pages jumps between them. The page rank of each web page is assigned initially and computed over and over again using the above formula until convergent. This computation can be done using matrices. Suppose that We use $l(i,j)$ to denote the probability that web page $j$ jumps to web page $i$, then We assign the values as follow:
    %     $\begin{cases}
    %     l(i,j)=0 \text{ if j has no hyperlink to i}\\
    %     l(i,j)= \frac{1}{L(j)} \text{ otherwise}
    %     \end{cases}$
        
    %     Suppose We have web page $1,2,...,N$, then We use matrix $\mathbb{R} = \begin{bmatrix}
    %     PR(p_1)\\
    %     PR(p_2)\\
    %     \vdots \\ 
    %     PR(p_N)
    %     \end{bmatrix} $ to denote the page ranks of the web pages $\{p1,...,p_N\}$. Then the formula to compute the page rank can be written as :
    %     $$ R = \begin{bmatrix}
    %     \frac{1-d}{N}\\
    %     \frac{1-d}{N}\\
    %     \vdots\\
    %     \frac{1-d}{N}
    %     \end{bmatrix} +d\begin{bmatrix}
    %     l(p_1,p_1),l(p_1,p_2),...,l(p_1,p_N)\\
    %     l(p_2,p_1), \ddots, \hfill , \vdots \\
    %     \vdots, \hfill, l(p_i,p_j), \\
    %     l(p_N,p_1), \hdots,\hfill , l(p_N,p_N)
    %     \end{bmatrix}R$$
    %     The idea of page rank is translated to text rank to find keywords in a text. To do that, We need to build a graph of relationships between words. We can simply build an undirected graph whose vertices is the word and whose edges is determined by the coocurrences of the words. Then the built graph can now mimics the web pages and the hyperlinks between them. So We can apply the page rank algorithm to compute a scoring for each word and select the top 5 highest scored words as the keywords for the text. If two keywords appear next to each other in the text, We can group them together as one keyword. See figure  ~\ref{fig:word_coocur_graph} for creating a word concurrence graph for concurrence of words within 5 words.
        
       
    %     \begin{figure}[h!]
    %       \centerline{\includegraphics[scale=0.6]{text_rank.JPG}}
    %       \caption{Creating word concurrence graph from a text. }
    %       \label{fig:word_coocur_graph}
    %     \end{figure} 

        
        
        
        
    %     \item (4) Industry classification calculation: We simply use the cosine similarity as the backbone of the computation. The cosine similarity between two vectors $x,y$ is $$cossim(x,y) = 1-\frac{x \cdot y}{\|x\| \cdot \|y\|}$$
    %     Supppose $w_0, w_1$ are the vector embedding of two words and We can compute the similarity between them as follow: $$ sim(w_0, w_1) = \frac{1}{cossim(w_0, w_1) \times avg(idf(w_0), idf(w_1))}$$. 
    %     We compute the similarity between all $ind(i,j,k)$ with all words in the text and denote the result by 
    %     $$ query_{i,x,y,z} = sim(v(i), ind(x,y,z))$$
    %     Finally, We use the Weaver-Thomas algorithm to compute the index$(i,x,y,z)$ in $query$ that best represent the $query$ and assign industry $x$ as the industry of the text. The Weaver-Thomas algorithm for finding an index of a sorted(from largest to smallest) list $X$ is given as: 
    %     $$\begin{cases}
    %     X(k*) \\
    %     k*= argmin(Q(k)), k \in \{1,2,...,N\}\\
    %     Q(k) = \sum_{j=1}^k[X(j) - \frac{100}{k}]^2 + \sum_{j=k+1}^N X^2(j)
    %     \end{cases}$$
        
    % \end{enumerate}
    \item The method We used here is a text classification deep learning method. We have used the pre-trained BERT ~\cite{Eckhoff:1979bi} model and pre-trained XLNET~\cite{Eck93survey} model and fine-tuned them on a training set. The URL for the BERT and the XLNET model can be found \href{https://arxiv.org/abs/1810.04805v2}{here} and \href{https://arxiv.org/abs/1906.08237v2}{here}. We chose the BERT model because it was the state-of-the-art model when it first came out and beat the previous state-of-the-art models in several natural language processing tasks.  The XLNET model was an improvement over the BERT model because it beat the BERT model in several natural language processing tasks.  
    
    We now explain the architecture of the BERT model. We will explain the process of how to train a BERT model from scratch. The BERT model is based on the transformer model and is pre-trained using two training methods, masked language modelling and next sentence prediction. 

For masked language modelling, We randomly select 15 percent of the tokens to be “masked". More precisely, out of the randomly selected 15 percent of the tokens, 80 percent of those would be replaced with the $[mask]$ token, 10 percent of them would be replaced by any other arbitrary token and 10 percent of them would be left unchanged. 

The formula that We are trying to maximize is 
$$logP(x_k|x-k;\theta) \approx \sum_{k\in K} logP(x_k|x-K;\theta) $$

When the sentence passes through the transformer encoder layer of the BERT model, We get an output of the shape $[BatchSize, SeqLen,EmbeddingDim]$. Then We multiply this result with a matrix of shape $[Emb_Dims, VocabSize]$ to get a distribution over all possible vocabulary that can then be used to calculate the loss.

For the next sentence prediction, We are given two sentences, and We want the BERT model to predict whether the second sentence follows the first sentence. What We do with the two sentences is first to add a $[cls]$ token at the beginning of the first sentence and join both sentences with a $[sep]$ token and end the joined sentence with a $[sep]$ token again. Those tokens are a special token that is designed for specific purposes. 

For training next sentence prediction, We have not only token embedding and positional embedding for each token, but also a segment embedding that is used to separate two sentences as illustrated in figure~\ref{fig:17}.: 

\begin{figure}[h!]
  \includegraphics[width=18cm]{cap39.JPG}
  \caption{Sentence encoding for BERT's next sentence prediction.}
  \label{fig:17}
\end{figure}

The segment encoding could be $0$ for the tokens in the first sentence and $1$ for the second sentence. 

By the nature of the computation of the self-attention mechanism, We know that information about the relation between the token in a sentence is computed for each token. In particular, We only need the relational information of the tokens with the first $[cls]$ token of the sentence to deduce useful information. Hence, in the output of the last transformer layer of the transformer encoder, We can use the first slice of the resulting matrix as the sole information needed to predict whether the two sentences have the correct ordering. 

And so, the output of the last transformer layer is of size $Z : [BatchSize, SeqLen, EmbeddingDim]$ and We just need to extract from it, the information about the $[cls]$ token by $cls_vector = Z[:0,:] \in \mathbb{R}^{[BatchSize, EmbeddingDim]}$. Moreover, finally, We do a linear transform followed by a sigmoid to get a probability score for the likelihood that the two sentences make sense semantically when putting them one after the other: $\hat{y} = sigmoid(Linear(cls_vector)) \in (0,1)$.

Now that We have explained the pertaining of the BERT model, We will see a small example of how to set up the pretraining process for the Chines language. First, We can get all the sentences of Wikipedia on a text file. We can make another txt file with pair of the sentence in the first txt file. The second text file will be used for the next sentence prediction training set. 

Then We can get all the words in the text together with their frequency of appearance to remove words that are not very frequent. 
After that, We need to create a dictionary that assigns tokens to an index value for token embedding lookup later on. We need to have several special tokens:
$\begin{cases}
[PAD] \text{ used to pad a sentence to the desired length.}\\
[UNK] \text{ used for words that are not seen before.}\\
[SEP] \text{ Used to separate two sentences.}\\
[CLS] \text{ used to denote the beginning of a sentence.}\\
[MASK] \text{ used to mask a token for masked language modelling}\\
[NUM] \text{ used to denote number.}
\end{cases}$


The text input to the model consists of the title, the description, the company name, the keywords, the URL of the web page, the content of the home page, the content of the about page, the content of the service page, the content of product page that We scrapped. The label is the first two digits of the six digits NAICS code. We trimmed the length of the text input to have at most 256 words. The total number of labels, in this case, is 25. We used 10000 companies from the GitHub repository to fine-tune the BERT model; 10 percent were used for validation and 90 percent used for training. We then fine-tuned the model to have a validation accuracy of 58 percent. A challenge to making the model more accurate is to choose what information on the web page to train the model. We used all of the information that We scraped from the web page as input to the model. It is a wrong choice of input text because sometimes, the length of the resulting text exceeds the capacity of the input requirement of the model, which is 512 tokens long. Another example of an issue is that the visible text description of the hyperlink of a web page does not contribute to determining the industry of the webpage.  Hence, We trained another model that only uses the sentences in the web page's content containing specific keywords. These keywords include the company name, "us," "we," "our." After fine-tuning the BERT model using the selected sentences only, our model has an accuracy increase of 3 percent. Note that We did not select sentences from the description and the title of the web page. We think that the web page's description and title are essential in determining its industry, so We do not need to select meaningful sentences further from them. We select important sentences only from home, the about, the product and the service page of the web page. 
    

\item We also used three traditional text classification methods. They are the Naive Bayes method, the support vector machine method and the recurrent neural network method. For the Naive Bayes and the support vector machine method, We gave the model as input the concatenation of the company name, the title of the web page, the description of the web page and the keywords of the web page. Then We transform the input text into a vector using the Scikit-learn TF-IDF vectorizer. We did a grid search for the best hyper-parameter for the models and tested the models on the testing set. 
\end{enumerate}

\subsection{Classification of Federally incorporated companies}



 \section{Results and discussion}
 \subsection{result}
 We will show the result of each of our methods now. We have used 3000 of the GITHUB data and 200 of the SICCODE agriculture data as our testing set and the rest of the data as the training set. 
 \begin{enumerate}
     \item The frequencies of the classified NAICS are shown in figure ~\ref{fig:21} ~\ref{fig:20}，~\ref{fig:29}，~\ref{fig:39}，~\ref{fig:40}:
        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.5]{cap46.jpg}}
          \caption{The frequency distribution of the Official labelled NAICS for the test web pages}
          \label{fig:21}
        \end{figure}
        
          \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.5]{manual_distribution.JPG}}
          \caption{The frequency distribution of the NAICS for the keyword method}
          \label{fig:20}
        \end{figure} 
        
        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.5]{svm_grid_searched_distribution.JPG}}
          \caption{The frequency distribution of the NAICS for the SVM method(grid searched)}
          \label{fig:29}
        \end{figure} 
        
        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.5]{svm2_dist.JPG}}
          \caption{The frequency distribution of the NAICS for the SVM version 2 method(grid searched)}
          \label{fig:41}
        \end{figure} 
        
        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.5]{naive_bayes_distribution.JPG}}
          \caption{The frequency distribution of the NAICS for the Naive Bayes method}
          \label{fig:39}
        \end{figure} 
        
        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.5]{rf_grid_searched_distribution.JPG}}
          \caption{The frequency distribution of the NAICS for the  random forest method(grid searched}
          \label{fig:40}
        \end{figure} 

        
     \item The classification report of the models are shown in figure ~\ref{fig:27} ~\ref{fig:bert_precision}，~\ref{fig:bert_precision_selected_sentences}，~\ref{fig:28}，~\ref{fig:38},
     ~\ref{fig:30},~\ref{fig:31}:
        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.5]{manual_key_acc.JPG}}
          \caption{The classification report for the classification by keyword and sentence compare}
          \label{fig:27}
        \end{figure}
        
        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.5]{bert_precision1.JPG}}
          \caption{The classification report for the BERT model using all contents}
          \label{fig:bert_precision}
        \end{figure}
        
        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.5]{}}
          \caption{The classification report for the BERT model using the selected sentences}
          \label{fig:bert_precision_selected_sentences}
        \end{figure}
        
        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.5]{svm_grid_searched_report.JPG}}
          \caption{The classification report for the classification by svm(grid searched)}
          \label{fig:28}
        \end{figure}
        
        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.5]{svmv2_report.JPG}}
          \caption{The classification report for the classification by SVM version 2(grid searched)}
          \label{fig:38}
        \end{figure}
        
        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.5]{naive_bayes_report.JPG}}
          \caption{The classification report for the classification by Naive Bayes method}
          \label{fig:30}
        \end{figure} 
        
        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.5]{rf_grid_searched.JPG}}
          \caption{The classification report for the classification by random forest method(grid searched)}
          \label{fig:31}
        \end{figure} 
     
 \end{enumerate}
 
 The accuracy of the models are shown in figures ~\ref{fig:35}, ~\ref{fig:36}:

        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.6]{keyword_Acc.JPG}}
          \caption{The classification accuracy for agriculture and no agriculture classification with keyword method}
          \label{fig:35}
        \end{figure} 
        
        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.6]{svm_acc.JPG}}
          \caption{The classification accuracy for agriculture and no agriculture classification with  SVM method}
          \label{fig:36}
        \end{figure} 
        
        \begin{figure}[h!]
          \centerline{\includegraphics[scale=0.6]{svm_acc.JPG}}
          \caption{The classification accuracy for agriculture and no agriculture classification with  SVM version 2 method}
          \label{fig:37}
        \end{figure} 



 
 \subsection{discussion}
 We will now discuss our project, highlight important points, and discuss our models' good aspects.
\begin{itemize}
    \item The labelled training data that we used to train our classification model has, in general, correct labelling for the industry. However, after examining the company where the predicted label and the given label differ, we have seen that some given labels were incorrect. Some given labels are partially correct, but some are quite different from what the company web page shows. We could assign a more justified label based on what we read on the company web page and the predicted label.  We selected two examples whose label was corrected by our model, and they are:
    
 \begin{enumerate}
     \item  Consider the company:
\begin{table}[h!]
\begin{tabular}{|l|l|}
\hline
name                 & Derma Secret Med Spa                                                                                                                                                                                                             \\ \hline
URL                  & www.dermasecret.ca                                                                                                                                                                                                               \\ \hline
NAICS                & 311811                                                                                                                                                                                                                           \\ \hline
Retail Bakeries      & Retail Bakeries                                                                                                                                                                                                                  \\ \hline
Web page description & \begin{tabular}[c]{@{}l@{}}We provide safe and affordable cosmetic \\ procedures from our locations in \\ MIssissauga and Waterloo. \\ Face, Body, Hair we have beauty solutions\\  so you can become the best you.\end{tabular} \\ \hline
\end{tabular}
\end{table}

If We go into the provided web page for this company, We see that the title of the web page is "Derma Secret Clinic | Cosmetic Beauty: Filler, Botox, Microneedling & More," and the description of the web page is  "We provide safe and affordable cosmetic procedures from our locations in Mississauga and Waterloo. Face, Body, Hair We have beauty solutions so you can become the best you." Hence, by the web page's description, this company is doing cosmetics and not retailing bakeries; therefore, the provided NAICS label is incorrect. 
      
      \item The company:
\begin{table}[h!]
\begin{tabular}{|l|l|}
\hline
name                 & The cleaning company                                                                                                                                                                 \\ \hline
URL                  & www.thecleaningcompany.ca                                                                                                                                                            \\ \hline
NAICS                & 236110                                                                                                                                                                               \\ \hline
Retail Bakeries      & Residential building construction                                                                                                                                                    \\ \hline
Web page description & \begin{tabular}[c]{@{}l@{}}The Cleaning Company is one of Toronto's leading \\ full-service cleaning agencies. \\ We offer a full range of specialty cleaning services.\end{tabular} \\ \hline
\end{tabular}
\end{table}
      
      So for this company, we believe its NAICS should be 561722 - Janitorial services (except window cleaning) as we see that the company is offering cleaning service in the description of the web page. So the provided NAICS 236110 residential building construction is incorrect. 
      
 \end{enumerate}
We give 16 additional examples of corrected labels in figure ~\ref{fig:company_correct_label}:
 \begin{figure}[h!]
  \centerline{\includegraphics[scale=0.6]{company_correction_list.PNG}}
  \caption{A list of Corrected labels by our program}
  \label{fig:company_correct_label}
\end{figure} 

\item  Next, the NAICS documents of version 2017v3 from the statistics Canada has missing labels. Several of the examples in the exclusion category has not been provided with true labelling. We will select a few of them. Each of the examples is shown in the comma separated format as "Level,Code,Class Title,Element Type Label English,Element Description English". We were able to remove around 30 examples that We cannot find a true label. 
 \begin{enumerate}
     \item 5,419110,Business-to-business electronic markets,Exclusion(s),acting as merchant wholesalers and bringing together buyers and sellers of goods using the Internet or a combination of electronic and traditional methods (classified according to the merchandise line or lines sold by the merchant wholesaler-distributor) 

     \item 2,419,"Business-to-business electronic markets, and agents and brokers",Exclusion(s),buying and selling goods on own account using a combination of electronic and traditional methods (classified according to the merchandise line or lines sold by the merchant wholesaler-distributor) 
     
     \item 5,332810,"Coating, engraving, cold and heat treating and allied activities",Exclusion(s),"fabricating, coating and engraving products (classified in the manufacturing sector according to the product made) "
     
     \item 5,334110,Computer and peripheral equipment manufacturing,Exclusion(s),manufacturing machinery or equipment that incorporate electronic computers for operation or control purposes and embedded control applications (other manufacturing industries based on the classification of the complete machinery or equipment) 
     
     \item 5,334110,Computer and peripheral equipment manufacturing,Exclusion(s),"manufacturing parts, such as casings, stampings, cable sets and switches, for computers and peripheral equipment (other manufacturing industries based on the associated production processes) "
     
     \item 5,454110,Electronic shopping and mail-order houses,Exclusion(s),store retailing or a combination of store retailing and Internet retailing of merchandise in the same establishment (classified to the store portion of the activity) 
     
     \item 5,561210,Facilities support services,Exclusion(s),"providing a single support service to clients, but not the range of services that establishments in this industry provide (classified according to the service provided) "
     
     \item 5,621610,Home health care services,Exclusion(s),in-home health services provided by health practitioners primarily engaged in the independent practice of their profession (classified with the profession) 
     


 \end{enumerate}
 \item We could use the NAICS definition and examples to help us create a list of keywords for each industry. However, manually augmenting the set of keywords by inspecting web pages can increase the model's accuracy.  
 
 
\end{itemize}
 


 

 
\subsection{Next Step}
A big challenge in web page classification is selecting the right information from the web page's content to feed into a classification model. It is challenging to have a more intelligent way to extract the right sentences from a web page relevant to the classification task. Hence this is an essential next step in developing a better web page classification algorithm.


\subsection{Conclusion}
There are several points that We want to enphasis:
\begin{enumerate}
          \item The challenge in determining that a web page is that of the company is that We are only given the previous and current company names and previous and current company addresses. From this information, We needed to determine whether a web page is that of the company or not.  We observed that if the web page's contact information section contains the current company address or the previous company address, then We are very confident that the web page is that of the company. Based on random scrapped samples, the correctness of the model in getting the correct company web page for a given company is 93\%. After finding the web page for the company, we then proceed to scrape the web page's content and store this information in the school database. 
          
          \item   Our goal for the internship is to create a program to find the web pages of a federally incorporated company by doing a Google search of its name. Then We need to determine from the web page's information if the company is agri-food related or not. We have done both with very high success. Our classification model can differentiate agri-food company and no agri-food company successfully. Our best model construct keywords for each industry using TF-IDF and manually select additional keywords from web pages. Then we use the keywords to determine the two digit NAICS of the web page. 
     
    \item We also took the opportunity to do additional work by making a model that can recognize more industries.
    \item Using all information on a web page, that includes the name of the company, the title of the web page, the description of the web page, the keywords of the web page, the content of the main page, the content of the about page, the content of product page and the content of the service page, to train a BERT text classification model will give an accuracy that can be further improved by selecting some key sentences to feed into the model. If instead of give to BERT the content of the main, about, product service page, We give to BERT the sentences that contains specific words such as the name of the company or "we" or "our", then the accuracy of the resulting trained model could improve by 3 percent.
    
        \item For our keywords method, if We simply use the NAICS Examples and keywords built using TF-IDF scoring from the NAICS definitions and examples, the accuracy of the resulting model is low as this rely on the capability of the model to understand a language. Instead, with the additional information of the labelled web pages, We can extract more meaningful manual keywords and that will help to increase the accuracy of the model. 
        
        \item We have learned many valuable experiences during our internship. We have learned to do group research together, which made us more confident in working together and discussing ideas and accomplishing a task. 
\end{enumerate}

 


% \begin{theorem}[Helge Tverberg 1966 \cite{Tverberg:1966tb}]
% Given $(r-1)(d+1)+1$ points in $\rr^d$, there is a partition of them into $r$ parts whose convex hulls intersect.
% \end{theorem}

% \begin{figure}
% \centerline{\includegraphics[scale=1]{fig1-Tverberg}}
% \caption{An example of a Tverberg partition.  The partition is not unique.}
% \end{figure}







\section{Acknowledgement}
We are fortunate to have as our supervisors professor Sandra Schillo and professor Nie Jian Yun. Throughout our internship, they provided helpful tricks and methods during our weekly meetings to our problems and helped us advance in the project. Equally, this project report could not have been done without their help. 

\section{Appendix}
\subsection{HTML}
In order to explain how the scrapping program work, We will first explain what HTML is. To see how this work, We open a text editor and type the following codes in it:
\begin{figure*}[h!]
  \centerline{\includegraphics[scale=1]{cap21.JPG}}
  \caption{HTML code}
  \label{fig:normal}
\end{figure*}

If We save the file with an extension .html and open the saved file using a web browser, We would get a web page that look like the following picture:
\begin{figure*}[h!]
  \centerline{\includegraphics[scale=1]{cap22.JPG}}
  \caption{HTML page example}
  \label{fig:normal}
\end{figure*}

A pair of enclosing $<name></name>$ for a text that looks like $<name>...some text...</name>$ is called a tag in the HTML language. As We see, the content inside the two tags of the form $<h1></h1>$ is rendered by the browser and the tags themselves are not show in the browser. They can come with a variety of properties that further tells the browser how the content of each tag should be displayed.  Each tag is used for a specific purpose, for example, the $<h1></h1>$ tag is telling the browser that the text in between the tags should be displayed to the user in bold or $<img></img>$ is telling the browser that there is an image that need to be displayed and is specified in the attributes of the tags. Namely, the displayed information on the web page lives insides tags in the HTML code and We need to tell a scrapping program which pair of tags， the information that We want resides, in order for the scrapping program to get that information for us. In HTML code, Always, We have the $<head></head>$ and $<body></body>$ tags that is enclosed by the $<html></html>$ tags. What We are most interested is the $<a>$ tag, the $<meta>$ tag and the $<title>$ tags. We explain the functionality of each of the tags one by one using the Strauss Feed web page example that can be accessed using at \href{https://www.straussfeeds.com}{\textcolor{blue}{this URL address}}. With modern day web browsers, it is possible that the the HTML source code of the web page can be examined by right click on the web page and click on inspect. We provide an example of getting the source code of a web page on the Edge web browser as shown on Figure ~\ref{fig:4}:
\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.3]{cap23.png}}
  \caption{HTML page example}
  \label{fig:4}
\end{figure}

Once We do, the user interface of the Edge web browser will be split into two halves left and right with one half containing information about the web pages. In that half, We can also see the source code of the web page as shown in Figure ~\ref{fig:5}::
\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.35]{cap24.JPG}}
  \caption{HTML page example with source code displayed}
  \label{fig:5}
\end{figure}


\begin{enumerate}
    \item The $<a>$ tag is used to put a hyperlink to link to other web pages. For example, if We want to let the user be able to click on the text ="link" to navigate to the URL Google.com, We can write $<a href ="https://www.google.com">text</a>$. It will be rendered by the browser as clickable text that will redirect the user to Google.com upon clicking on it. We are interested in this tag because of the different pages such as the about page or the contact page of the website. 
    
    \item The $<meta>$ tag is usually inside the $<head>$ tag and will contain useful information about the web page that We can use to do industry classification. Consider the HTML source code of the Strauss Feed web page in figure ~\ref{fig:5}, We can see that there is a meta tag with the name  equals to "description" in the highlighted code. The content of the tag can be seen in fig ~\ref{fig:7}
    \begin{figure}[h!]
      \centerline{\includegraphics[scale=0.5]{cap27.JPG}}
      \caption{meta tag of Strauss feed web page}
      \label{fig:7}
    \end{figure}
    
    \item The $<title>$ tag resides in the $<head>$ tag section of the HTML code and will give a title to the web page. This is helpful for industry classification as the title can be indicative of the kind of business the web page is promoting. The title can be seen in the browser as the text displayed on the tab for that web page. It suffices to hover the mouse over the tab for the web page to see the information displayed on the tab. In the Strauss feed web page example, the title can be seen as "Strauss Feeds - Quality Milk Replacers for Infant Animals" as We can see in figure ~\ref{fig:6} 
    \begin{figure}[h!]
      \centerline{\includegraphics[scale=0.35]{cap26.png}}
      \caption{HTML page example with title displayed}
      \label{fig:6}
    \end{figure}
    
\end{enumerate}
\subsection{Internet protocol}
Now that We have a basic understanding of HTML and saw how a browser can be used to render the HTML code of a web page, We proceed to explain the basics of internet and how a python program could be used to fetch the content of a web page on the Internet. As We see previously, a company web page that is rendered by the browser is a HTML file containing HTML codes. This file resides on the company's computer. The company computer that store the files that is accessible on the internet is called a server. Then the entire process of viewing a web page can be thought of as a user makes a request to a server for a specific file and the server send that file to the intended recipient. How those files are transmitted to the correct destination and how to request the desired server for the particular file is what internet protocol is for. 
To illustrate, We can see the flow of data with the web browser's integrated package capturing tools. This can be viewed in the same way to look for a web page source code. We can see the different requests that the computer has made to the server in the network tag when inspecting a web page and We can take a look at figure~\ref{fig:13}.
\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.38]{cap35.JPG}}
  \caption{Network tab of web page inspection}
  \label{fig:13}
\end{figure}
There, our local computer first makes a request to the internet to fetch the web page https://www.straussfeeds.com/, then it realises that in order to render the page, it needs the modern.js file and so it makes a requests to fetch that file again. It then proceed to request and fetch all necessary files to render the web page and then render the straussfeeds main page. The infomations on the location of the server can be found in the header section of a request. This can also be viewed in the browser by click on a requested item and select the header tag as shown in figure:

figure~\ref{fig:14}.
\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.48]{cap36.JPG}}
  \caption{Header section of a request}
  \label{fig:14}
\end{figure}

In the headers section of the request, there is a field called "user-agent", this field contains information about the browser that is requesting the file to the server. As server has ways to verify the requesting machine, a scrapping program could be clocked by the server from getting a file. To deal with this, the program could append the header information of a browser and make a request to the server pretending to be a browser. 


\subsection{transformer}
In this section, We will explain the architecture of the transformer model. Its architecture can be views in the figure ~\ref{fig:13} that We borrowed from the transformer paper \cite{Tverberg:1966tb} itself.

\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.6]{1_JuGZaZcRtmrtCEPY8qfsUw.png}}
  \caption{Transformer architecture}
  \label{fig:13}
\end{figure}

The model consists of a encoder with several layers of multi-head attention computation and a decoder of with several layers of multi-head attention computation. The transformer model is the basis of the more recent state of the arts natural language model and understanding its inner working would help us understand the more recent natural language model. We will use an example to illustrate the flow of a sentence through the model's encoder. Suppose We have a sentence "The dog is sleeping", each words in the sentence is first transformed into a vector $x_{the},x_{dog},x_{is},x_{sleeping}$. and each of the transformed vector will be multiplied by three matrices $W^Q,W^K,W^V$ to get matrices $x_{the,Q},x_{the,K},x_{the,V}...$. We can stack the vectors into matrices $Q,K,V$ as shown in figure ~\ref{fig:10}. 

\begin{figure}[h!]
  \centerline{\includegraphics[scale=0.6]{cap31.JPG}}
  \caption{Transformer word flow part 1}
  \label{fig:10}
\end{figure}

There are three vectors that We associate with each word like the $x_{the,Q},x_{the,K},x_{the,V}$ that We computed above. There are referred to as the query, key and value vector for that word.
Next, We need to compute the self attention with the computed matrices. The idea of self attention is to give a numerical meaning to the relationship between words in a sentence. For each word, We want to compute its relation with every other words in the sentence. We do so by taking a weighted sum of the value vectors for every words in the sentence. And the weight will be calculated by the dot product of the query vector of the word with the key vectors of every words. We first take a look the the matrix computation in figure ~\ref{fig:11}. 

\begin{figure}[h!]
  \includegraphics[width=13cm]{cap33.JPG}
  \caption{Transformer word flow part 2}
  \label{fig:11}
\end{figure}

In the figure, the first row of the matrix $Z$ represent the weighted sum of the value vectors of all words in the sentences. And the weight can be clearly seen as the soft-max of quotient of the product of the row in the query matrix representing the word with the key matrix by the square-root of the dimension of the query vector. The matrix $Z$ is the result of one computation of self attention. Self attention is the term used for this computation of relationship between the words in the sentence. In multi-headed attention, We would have more than one computation of self attention and so there would be more than one set of matrices $(W^Q, W^K, W^T)$. So if We had 8 heads attention, We would compute 8 matrices $Z_1,...,Z_8$. After We computed all the $Z_i$ matrices, We can stack them and feed them into a linear transformation to get a matrix $Z$ that has the same dimension as the input matrix $X$. We can add $Z$ and $X$ and do a layer normalization and feed the result into a linear transformation again to produce the final output of one layer of the encoder of the transformer.  These steps are illustrated in figure~\ref{fig:12}.
\begin{figure}[h!]
  \includegraphics[width=13cm]{cap34.JPG}
  \caption{Transformer word flow part 3}
  \label{fig:12}
\end{figure}

We have now the flow of a sentence through the encoder of the transformer. We write down the mathematical formulations:
\begin{enumerate}
    \item $X = Embedding_Lookup(X)+Positional_Encoding(X)$
    \item $\begin{cases}
          Q = X*W^Q \\
          K = X*W^K \\
          V = X*W^V
          \end{cases}$
    \item $X_{attention} = X+X_{attention}$
    \item $X_{attention} = layer_norm(X_{attention})$
    \item $X_{hidden} = X_{attention}$
\end{enumerate}

% \subsection{page rank}
% \begin{definition}
% A Markov chain has a finite set of states and for each pair of states $x,y$ there is a transition probability $p_{xy}$ of going from state $x$ to state $y$ and We have $\sum_y p_{xy} = 1$.
% \end{definition}

% A Markov chain can be used to model a user navigating the word web from one web page to another with a certain probability using the hyperlinks between the web page.  This is formalized as a random walk on a Markov chain

% \begin{definition}
% A random walk in the Markov chain start at some state $x$ . At any given time, it moves to a state $y$ with probability $p_{xy}$.
% \end{definition}

% We can model a Markov chain using a directed graph where each state of the Markov chain can be represented by a node in the directed graph and there is a directed edge of weight $p_{xy}$ from node $x$ to node $y$.

% \begin{definition}
% A Markov chain is called connected if the underlying directed graph is strongly connected. 
% \end{definition}

% \begin{definition}
% Given a Markov chain is a random walk on it. Let $P^{t}$ denote the probability distribution after $t$ steps of random walk. The \textbf{long-term probability distribution} is defined as  $a^{(t)} = \frac{1}{t}(p^{(0)}+p^{(1)}+...+p^{(t-1)})$
% \end{definition}

% \begin{lemma}
% Given a transition matrix $P$ of a Markov chain with $n$ states, The $n \times (n+1)$ matrix $A :=[P-I,\mathbb{1}] $ has rank $n$.
% \end{lemma}
% \begin{proof}

% \end{proof}


\subseection{Classification Code}




\subsection{MPnet}
MPnet is introduced by  Song et.al to deal with the shortcoming of BERT and its successor XLnet by combining the best of both models. The masked language modelling objective of BERT doesn't take into account the relation between masked words. Hence XLnet is pretrainined using the permuted language modelling to deal with this shortcoming of BERT. However, the permuted language modelling has the problem that the token cannot see the positional information of the whole sentence. Then MPnet unified both training objective under the name masked and permuted language modelling. 

Here is the idea of masked and permuted language modelling.  Consider a sequence of length $6$, $(x_1,x_2,x_3,x_4,x_5,x_6)$. We pick a random permutation of the sequence say $z=(1,3,5,4,6,2)$ and rearrange the sequence according to this permutation to get the sequence $(x_1,x_3,x_5,x_4,x_6,x_2)$. We choose a length for the non-predict part say $c= 3$ and denote the non-predict part of the sequence by $ x_{z<c}=(x_1,x_3,x_5)$ and the predict part by $x_{z>c}=(x_4,x_6,x_2)$. Then We create a new input tokens by adding masks right before the predicted part to get $(x_{z<=c}, M_{z>c}, x_{z>c}) = (x_1,x_3,x_5,[M],[M],[M],x_4,x_6,x_2)$ 

\subsection{Scrapping Code}
\begin{mintedbox}{python}
############################################
# The imports of the needed libraries
############################################
import unidecode
import json
from lxml import html
import pymongo
import re
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
from bs4.element import Comment
from urllib.request import Request, urlopen
import pickle
try:
    from googlesearch import search
except ImportError:
    print("No module named 'google' found")

############################################
# Constants to be set
############################################
RUNNING_FIRST_TIME = True # running the code for the first time used to not search from the first company if the program gets interrupted
INSERT_NO_FOUND_WEBPAGE_COMPANY = False # whether to insert company that we didn't find a web page in the database

############################################
# The header of the scrapping files
############################################
headers = {
    "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36 Edg/90.0.818.62"
}

############################################
# Run this if running the program for
# the first time
############################################
if RUNNING_FIRST_TIME:
    with open('0_to_50000.pkl', 'wb') as file:
        pickle.dump(0, file)
    with open('0_to_50000_skipped_companies.pkl', 'wb') as file:
        pickle.dump([], file)

############################################
# Webpage that often returned by Google
# that needs to be ignored because they are
# other form of database or search engin
############################################
trap_websites = {
    "www.usaopps.com",
    "www.dandb.com",
    "www.manta.com",
    "www.buzzfile.com",
    "m.facebook.com",
    "siccode.com",
    "www.producemarketguide.com",
    "opengovca.com",
    "www.can1business.com",
    "websites.ca",
    "www.fido.ca",
    "www.yellowpages.ca",
    "www.canadapost.ca",
    "federalcorporation.ca",
    "montreal.ca",
    "www.ledevoir.com",
    "www.facebook.com",
    "www.realtor.ca",
    "www.realtor.com",
    "opengovca.com",
    "ca.indeed.com",
    "residences-quebec.ca",
    "www.linguee.com",
    "www.zoominfo.com",
    "www.gov.pe.ca",
    "www.princeedwardisland.ca",
    "www.postal-addresses.ca",
    "www.ville.dorval.qc.ca",
    "emplois.ca.indeed.com",
    "www.northislandgazette.com",
    "facebook.com",
    "procam.ca",
    "www.autocompanies.org",
    "www.pagesjaunes.ca",
    "ville.montreal.qc.ca",
    "www.royallepage.ca",
    "www.canadaregistry.org",
    "www.portailconstructo.com",
    "www.fr.canada411.ca",
    "www.journaldelevis.com",
    "www.transports.gouv.qc.ca",
    "opencorpdata.com",
    "deborahreed.m", # federal corp lookup
    "cincinnatibusinesslist.com", #public corp data lookup
    "duproprio.com",
    "numerique.banq.qc.ca", # library website
    "www.restomontreal.ca", # restaurent lookup website
    "ca.linkedin.com",
    "issuu.com",
    "www.dnb.com",
    "en.wikipedia.org",
    "www.postalcodesincanada.com",
    "opencorporates.com", # another compagny database
    "www.yelp.ca",
    "www.findglocal.com",
    "ww.weatheravenue.com",
    "reglements.ville.quebec.qc.ca",
    "bonjourresidences.com",
    "quoifaireaquebec.com",
    "www.bizapedia.com",
    "orthodontistesquebec.com",
    "fr.yelp.ca",
    "canada-corp.com",
    "www.canpages.ca",
    "fr-ca.facebook.com",
    "www.michaelpage.ca",
    "www.tripadvisor.ca",
    "www.ubereats.com",
    "www.ville.mont-royal.qc.ca",
    "www.codepostalmonde.com",
    "canada247.info",
    "local.fedex.com",
    "www.canada.ca",
    "gazette.gc.ca",
    "www.canada411.ca",
    "www.companiesofcanada.com",
    "www.honestdoor.com",
    "www.booking.com",
    "use.infobelpro.com",
    "www.companylisting.ca",
    "www.expedia.ca",
    "www.whitepagescanada.ca",
    "www.ville.quebec.qc.ca",
    "www.canadacompanyregistry.com",
    "www.canadacorporation.info",
    "www.canadacompanysearch.com",
    "411.ca",
    "www.yelp.com",
    "www.registreentreprises.gouv.qc.ca",
    "tt.linkedin.com",
    "www.tripadvisor.com",
    "www.hotelplanner.com",
    "www.registreentreprises.gouv.qc.ca",
    "nl.wikipedia.org",
    "www.211qc.ca",
    "www12.statcan.gc.ca",
    "deborahreed.me",
    "local.infobel.ca",
    "www.datalog.co.uk",
    "fr-fr.facebook.com",
    "nline.flippingbook.com",
    "www.infoentrepreneurs.org",
    "canada-company.com",
    "www.bbb.org",
    "www.linkedin.com",
    "www.resourceconnector.ca",
    "211north.ca",
    "london.ctvnews.ca",
    "ampq.ca",
    "business.facebook.com",
    "www.louer.ca",
    "www.mapquest.com",
    "www.walkscore.com",
    "www.loopnet.com",
    "en.parkopedia.ca",
    "www.opentable.com",
    "www2.ville.montreal.qc.ca",
    "www.centris.ca",
    "fr.wikipedia.org",
    "rafburtonwoodbase.org",
    "www.trouverunentrepreneur.com",
    "www.youtube.com",
    "www.opentable.com",
    "www.construction411.com",
    "fr.cylex-canada.ca",
    "www.animationdirectory.ca",
    "www.travelweekly.com",
    "www.trip.com",
    "rentitornot.com",
    "www.rentseeker.ca",
    "https://www.zillow.com/",
    "www.quebec-entreprises.com",
    "www.donneesquebec.ca",
    "www.apple.com",
    "docplayer.fr",
    "www.remax.ca",
    "www.zumper.com",
    "www.redfin.ca",
    "www.telephoneinverse.com", # phone lookup
    "pt.yellowpages.net",
    "www.anugo.ca",
    "www.linkedin.cn",
    "espresso-jobs.com",
    "www.chamber-commerce.net",
    "cancompany.org",
    "quebec-companies.com",
    "ca.jobsmarket.io",
    "www.local.ch",
    "www.maharashtradirectory.com",
    "https://www.foodindex.biz/",
    "cancompany.org",
    "www.quebecentreprises.com",
    "opengovca.com",
    "iphone.facebook.com",
    "www.canadabusinessportal.com",
    "www2.gov.bc.ca",
    "corporationscanada.ic.gc.ca",
    "www.dealerrater.ca",
    # bad sites
    "studylib.net",
    "health-products.canada.ca" # temporarly unavalable
}

############################################
# Code to insert an intem in the database
############################################
def inserRowDb(col0, current_comp_name, comp_names, best_url0, active0, comp_address,comp_addresses, comp_id0, ttt0, ddd0, body0, aboutt0, products0, kkk0,
    contact0, img0, hasaddress, has_prev_addresses,service, directory, search_name_in_url, search_name_in_title,business_no, abbraviation_match, full_name_match,logo, search_name):
    item ={}
    item["current_company_name"] = current_comp_name
    item["company_names"] = comp_names
    item["url"] = best_url0
    item["active"] = active0
    item["current_address"] = comp_address
    item["company_addresses"] =  comp_addresses
    item["corp_id"] = comp_id0
    item["title"] = ttt0
    item["description"] = ddd0
    item["main_page_content"] = body0
    item["about_page_content"] = aboutt0
    item["product_page_content"] = products0
    item["keywords"] = kkk0
    item["contact_page_content"] = contact0
    item["images"] = img0
    item["has_current_address"] = hasaddress
    item["has_previous_addresses"] = has_prev_addresses
    item["service_page"] = service
    item["directory_page"] = directory
    item["search_name_in_url"] = search_name_in_url
    item["search_name_in_title"] = search_name_in_title
    item["business_number"] = business_no
    item["search_name_abbraviation_doamin_match"] = abbraviation_match
    item["search_name_domain_match"] = full_name_match
    item["logo"] = logo
    item["web_page_searched name"] = search_name
    try:
        col0.insert_one(item)
    except pymongo.errors.DuplicateKeyError:
        # skip document because it already exists in new collection
        print("cannot insert item into db")

############################################
# Code to get the title, description,
# keywords and logo picture link from the
# head section of the web page.
############################################
def get_web_head_info(url):
    title = ""
    logo = ""
    try:
        res = requests.get(url, headers=headers,timeout=10)
    except Exception as e:
        print(e)
        return ["", "", ""]
    t = ""
    try:
        t = html.fromstring(res.content)
    except Exception as e:
        print(e)
        return ["", "", ""]
    try:
        title = (t.xpath('/html/head/title/text()')[0]).strip()
        metas = t.xpath('/html/head/meta')
    except Exception as er:
        print(er)
        return ["", "", ""]
    keywords = ""
    description = ""
    # we follow https://gist.github.com/lancejpollard/1978404 as
    # list of possible meta tags
    for meta in metas:
        if meta.xpath("@name") != [] and (meta.xpath("@name")[0]).lower() == "description" and description=="":
            try:
                description = (meta.xpath("@content")[0]).strip()
            except Exception as er:
                print(er)
        if meta.xpath("@name") != [] and (meta.xpath("@name")[0]).lower() == "twitter:description" and description=="":
            try:
                description = (meta.xpath("@content")[0]).strip()
            except Exception as er:
                print(er)
        if meta.xpath("@name") != [] and (meta.xpath("@name")[0]).lower() == "og:description" and description=="":
            try:
                description = (meta.xpath("@content")[0]).strip()
            except Exception as er:
                print(er)
        if meta.xpath("@property") != [] and (meta.xpath("@property")[0]).lower() == "og:description" and description=="":
            try:
                description = meta.xpath("@content")[0]
            except Exception as er:
                print(er)
        if meta.xpath("@name") != [] and (meta.xpath("@name")[0]).lower() == "keywords" and keywords == "":
            try:
                keywords = meta.xpath("@content")[0]
            except Exception as er:
                print(er)
        if meta.xpath("@property") != [] and (meta.xpath("@property")[0]).lower() == "og:image" and logo == "":
            try:
                logo = meta.xpath("@content")[0]
            except Exception as er:
                print(er)
        if meta.xpath("@name") != [] and (meta.xpath("@name")[0]).lower() == "twitter:image" and logo == "":
            try:
                logo = meta.xpath("@content")[0]
            except Exception as er:
                print(er)
        if meta.xpath("@name") != [] and (meta.xpath("@name")[0]).lower() == "og:image" and logo == "":
            try:
                logo = meta.xpath("@content")[0]
            except Exception as er:
                print(er)
        else:
            continue
    return [title, keywords, description,logo]


############################################
# Code to check if the company name
# abbriviation and company full name is the
# same as the domain name of the webpage.
############################################
def abbraviation_full_match(comp_name_as_list, domain_name):
    filtered = []
    temp = ""
    for n in comp_name_as_list:
        if len(n) > 1:
            lowered = unidecode.unidecode(n.lower()) # convert no ascii char to corresponding ascii char
            filtered.append(lowered)
            temp += lowered[0]
    if temp == domain_name:
        return True
    return False

def comp_name_full_match(comp_name, domain_name):
    filtered = []
    for n in comp_name:
        if len(n) > 1:
            filtered.append(unidecode.unidecode(n.lower()))
    temp = "".join(filtered)
    if temp == domain_name:
        return True
    return False



############################################
# Code to check if the company has been
# deactivated or not
############################################
def check_federal_corp(
        corp_id):  # https://www.ic.gc.ca/app/scr/cc/CorporationsCanada/fdrlCrpDtls.html?corpId=232203&V_TOKEN=null&crpNm=&crpNmbr=232203&bsNmbr=
    query_url = f"https://www.ic.gc.ca/app/scr/cc/CorporationsCanada/fdrlCrpDtls.html?corpId={corp_id}&V_TOKEN=null&crpNm=&crpNmbr={corp_id}&bsNmbr="
    disolved = ""
    try:
        res = requests.get(query_url, headers=headers, timeout=10)
    except Exception as e:
        print(e)
        return ""
    t = html.fromstring(res.content)
    try:
        disolved = (t.xpath('/html/body/main/div[1]/div/div/div/section/div[4]/div[4]/div[2]/text()')[0]).strip()
    except Exception as er:
        print(er)
        return False
    return "active" in disolved.lower() and ("inactive" not in disolved.lower())

############################################
# The following three method help are codes
# to get the visible texts from the web page
############################################
def tag_visible(element):
    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:
        return False
    if isinstance(element, Comment):
        return False
    return True

def text_from_html(body):
    soup = BeautifulSoup(body, 'html.parser')
    texts = soup.findAll(text=True)
    visible_texts = filter(tag_visible, texts)
    return u" ".join(t.strip().replace("\n","").replace("\t","").replace("  ","") for t in visible_texts)


def get_body_content(link):
    try:
        req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})
        html = urlopen(req, timeout=10).read()
    except Exception as e:
        print(e)
        return ""
    return text_from_html(html)

############################################
# Code to get the content of a link with
# a specific name such as about or contact
############################################
def description_from(url, section_str):
    res = ""
    try:
        res = requests.get(url, headers=headers, timeout =10)
    except Exception as e:
        print(e)
        return ""
    soup_data = BeautifulSoup(res.text, 'html.parser')
    a_tags = soup_data.find_all('a')
    result = ""
    gotit = False
    for a_tag in a_tags:
        if gotit:
            break
        a_tag_processed_text = unidecode.unidecode(
            a_tag.text.strip().lower().replace(" ", "").replace("\n", "").replace("-", ""))
        try:
            a_tag_url = a_tag["href"]
        except Exception as e:
            print(e)
            continue
        if a_tag_processed_text in section_str and a_tag_processed_text != "" and len(a_tag_processed_text) != 1:
            try:
                if a_tag_url[0:4] != "http" and a_tag_url[0] != "/":
                    domain = urlparse(url).netloc
                    bla = "https://" + domain +"/"+ a_tag_url
                    bla2 = "http://" + domain +"/"+ a_tag_url
                elif a_tag_url[0:4] != "http" and a_tag_url[0] == "/":
                    domain = urlparse(url).netloc
                    bla = "https://" + domain + a_tag_url
                    bla2 = "http://" + domain + a_tag_url
                else:
                    bla = a_tag_url
                    bla2 = ""

            except Exception as e:
                print(e)
                continue
            result += get_body_content(bla)
            result += get_body_content(bla2)
            gotit=True
    return result

############################################
# Code to get a list of a federally incorporated
# company directors and their addresses from
# its corporation id
############################################
def get_directors_infomation(corp_id):  # https://www.ic.gc.ca/app/scr/cc/CorporationsCanada/fdrlCrpDtls.html?corpId=232203&V_TOKEN=null&crpNm=&crpNmbr=232203&bsNmbr=
    query_url = f"https://www.ic.gc.ca/app/scr/cc/CorporationsCanada/fdrlCrpDtls.html?corpId={corp_id}&V_TOKEN=null&crpNm=&crpNmbr={corp_id}&bsNmbr="
    directors_list= []
    addresses_list = []
    try:
        res = requests.get(query_url, headers=headers, timeout=10)
    except Exception as e:
        print(e)
        return [],[]
    t = html.fromstring(res.content)
    try:
        director_lists = t.xpath('/html/body/main/div[1]/div/div/div/section/div[8]/div[2]/ul/li')
    except:
        return [],[]
    for k in director_lists:
        director_info = k.text_content().strip().replace("\t","")
        director_info = director_info.split("\n")
        directors_list.append(director_info[0])
        addresses_list.append(director_info[2])
    return directors_list, addresses_list


############################################
# Code to check if the address is in the
# provided text.
############################################
def isAddressInPage(add_str, content_str):
    c_str = content_str.lower()
    test = re.compile(r'[A-Z]\d[A-Z] *\d[A-Z]\d')
    try:
        postal_code = test.search(add_str).group()
        postal_code_ns = postal_code.replace(" ", "")
        if postal_code[3] != " ":
            postal_code = postal_code[0:3]+" "+postal_code[3:]
        numbers = (re.findall('\d{2,8}', add_str)[0]).strip()
    except Exception as e:
        print(e)
        return False
    return ((postal_code.lower() in c_str) and (numbers in c_str)) or ((postal_code_ns.lower() in c_str) and (numbers in c_str))

def containsAddress(web_url, address):
    getpage = requests.get(web_url)
    getpage_soup = BeautifulSoup(getpage.text, 'html.parser')
    all_links = getpage_soup.findAll('a')
    try:
        contents = get_body_content(web_url)
        if isAddressInPage(address, contents):
            return True
    except Exception as e:
        print("main page accessing error:",e)
    print("checking other pages")
    for a_tag_url in all_links:
        a_tag_url = a_tag_url.get("href")
        try:
            if a_tag_url[0:4] != "http" and a_tag_url[0] != "/":
                domain = urlparse(web_url).netloc
                bla = "https://" + domain + "/" + a_tag_url
                bla2 = "http://" + domain + "/" + a_tag_url
            elif a_tag_url[0:4] != "http" and a_tag_url[0] == "/":
                domain = urlparse(web_url).netloc
                bla = "https://" + domain + a_tag_url
                bla2 = "http://" + domain + a_tag_url
            else:
                bla = a_tag_url
                bla2 = ""
        except Exception as e:
            print(e)
            continue
        result = ""
        result += get_body_content(bla)
        result += get_body_content(bla2)
        if isAddressInPage(address, result):
            return True
    return False

############################################
# Code to remove the trailling part of
# a company name such as ltd or inc.
############################################
def preprocess_name(name):
    the_comp_name = name.lower()#.replace(" ", "")
    if len(the_comp_name) >= 5 and the_comp_name[-5:-1] == "ltee":
        the_comp_name = the_comp_name[:-5]
    elif len(the_comp_name) >= 5 and the_comp_name[-5:-1] == "inc.":
        the_comp_name = the_comp_name[:-5]
    elif len(the_comp_name) >= 5 and the_comp_name[-5:-1] == "ltd.":
        the_comp_name = the_comp_name[:-5]
    elif len(the_comp_name) >= 8 and the_comp_name[-8:-1] == "limited":
        the_comp_name = the_comp_name[:-8]
    elif len(the_comp_name) >= 4 and the_comp_name[-4:-1] == "inc":
        the_comp_name = the_comp_name[:-4]
    elif len(the_comp_name) >= 5 and the_comp_name[-5:-1] == "corp":
        the_comp_name = the_comp_name[:-5]
    elif len(the_comp_name) >= 4 and the_comp_name[-4:-1] == "ltd":
        the_comp_name = the_comp_name[:-4]
    elif len(the_comp_name) >= 12 and the_comp_name[-12:-1] == "corporation":
        the_comp_name = the_comp_name[:-12]
    return the_comp_name

############################################
# Code to get the a list of images's link
# from a given web page.
############################################
def image_list(url):
    r = ""
    try:
        r = requests.get(url, timeout =10)
    except:
        return []
    soup = BeautifulSoup(r.text, 'html.parser')
    images = soup.findAll('img')
    img_list = []
    if len(images) != 0:
        for i, image in enumerate(images):
            try:
                image_link = image["data-srcset"]
            except:
                try:
                    image_link = image["data-src"]
                except:
                    try:
                        image_link = image["data-fallback-src"]
                    except:
                        try:
                            image_link = image["src"]
                        except:
                            pass
            img_list.append(image_link)
    return img_list

import urllib.request
############################################
# Code to check if the internet is connected
############################################
def connect(host='http://google.com'):
    try:
        urllib.request.urlopen(host) #Python 3.x
        return True
    except:
        return False

############################################
# Setup the database connection and
# pre scrapping preparations
############################################
myclient = pymongo.MongoClient("localhost:27017")
mndb = myclient["local"]
mycol = mndb['new_scrap_federal']
mycol.create_index([('corp_id',pymongo.ASCENDING)], unique= True ) # to avoid putting duplicates

with open('./federal_corporation_list_raw_2021_08_17.json', 'r', encoding="utf-8") as myfile:
    comp_data=myfile.read()
federal_company_list = json.loads(comp_data)

count = 0
processed_entries = 0
skipped_companies_due_to_error = []
with open('0_to_50000.pkl', 'rb') as file:
    processed_entries = pickle.load(file)
with open('./0_to_50000_skipped_companies.pkl', 'rb') as file:
    skipped_companies_due_to_error = pickle.load(file)


############################################
# Code to Google search the company web
# page using company name and store
# scrapped information on the database
############################################
for entry in federal_company_list:
    # check if alowed range of samples has been scrapped
    if count < processed_entries:
        count += 1
        continue
    print("..........Processing the ",count,"th company..........")
    # check for internet connection, stop scrapping when no internet
    if connect():
        print("Connected to the internet")
    else:
        print("Not connected to the internet... break")
        break
    if len(entry["names"]) == 0 and entry["current_name"]=="":
        print("the company has no name, we skip...")
        if INSERT_NO_FOUND_WEBPAGE_COMPANY:
            inserRowDb(mycol, entry["current_name"], entry["names"], "", False, entry["current_address"],
                       entry["addresses"], entry["corp_id"], "", "", "", "", "", "",
                       "", "", False, False, "", "", False,
                       False, entry["business_n0"],False,False,"","")
        continue

    found_web_page = False
    isAlive = False
    try:
        isAlive = check_federal_corp(entry["corp_id"])
    except Exception as e:
        print("CHECK FEDERAL CORP ERROR:",e,"...Skipping...")
    directors, directors_addresses = [],[]
    try:
        directors, directors_addresses = get_directors_infomation(entry["corp_id"])
    except Exception as e:
        print("GET DIRECTORS INFOR ERROR: ",e,"...Skipping getting director information...")

    for the_comp_name in [entry["current_name"]] + entry["names"]: # iterate over all possible names
        if not found_web_page: # if found webpage, then we stop
            comp_name_list = re.split("[, -']", str(the_comp_name).lower())
            comp_name_list_without_traillling = comp_name_list[:]
            if (comp_name_list_without_traillling[-1] in {"ltee", "inc.", "ltd.", "limited", "inc", "corp", "ltd", "corporation"}):
                comp_name_list_without_traillling.pop()
            print("company name as a list is:", comp_name_list)
            title = ""
            description = ""
            keywords = ""
            about = ""
            main = ""
            product = ""
            contact = ""
            directory = ""
            service = ""
            logo = ""
            img = ""
            domain_url_name_only = ""
            has_current_address = False
            has_addresses = False
            search_name_in_title =False
            search_name_in_url = False
            abbraviation_name_url_match = False
            name_domain_ful_match = False

            try:
                for j in search("webpage of " + str(the_comp_name), tld="ca", num=6, stop=6, pause=6):
                    if domain_url in trap_websites:
                        continue
                    domain_url = urlparse(j).netloc
                    print("found URL: ",j," domain: ", domain_url)
                    domain_url_name_only = ".".join((domain_url.split("."))[:-1])
                    if len(domain_url_name_only) >= 3 and domain_url_name_only[0:3] == "www":
                        domain_url_name_only = domain_url_name_only[3:]
                    abbraviation_name_url_match = abbraviation_full_match(comp_name_list, domain_url_name_only) or abbraviation_full_match(comp_name_list_without_traillling, domain_url_name_only)
                    name_domain_ful_match = comp_name_full_match(comp_name_list,domain_url_name_only) or comp_name_full_match(comp_name_list_without_traillling,domain_url_name_only)
                    title, keywords, description, logo = get_web_head_info("https://" + domain_url)
                    about = description_from("https://" + domain_url, "|about|aboutus|quisommesnous|introduction|ourstory|howitworks|ourmission|info|company|aboutme|")
                    main = get_body_content("https://" + domain_url)
                    products = description_from("https://" + domain_url, "|product|products|ourproducts|featuredproducts|shop|shopping|deals|shopall|shophair|allproducts|clearance|")
                    contact = description_from("https://" + domain_url, "|connectwithus|contactez-nous|nousjoindre|contact|home|contactus|accueil|getintouch|yourchamber|contactthebia|accueil|corporate|getintouch|")
                    directory = description_from("https://" + domain_url,
                                                      "|staff|directory|ourteam|ourdirectory|ourstaff|team|ourpeople|people|servicesettarifs|boardofdirectors&staff|directors|boardofdirectors|executivemanagementteam|")
                    service = description_from("https://" + domain_url,
                                                    "|services|intergrationservices|ourservices|serviceplans|")
                    try:
                        img = "|".join(image_list("https://" + domain_url))
                    except Exception as e:
                        print("Error wile trring to get images of the webpage",e)


                    number_of_company_addresses = len(entry["addresses"])
                    if number_of_company_addresses >= 1:
                        for i in range(number_of_company_addresses):
                            if containsAddress("https://" + domain_url, entry["addresses"][i]):
                                has_addresses = True
                    if containsAddress("https://" + domain_url, entry["current_address"]):
                        has_current_address = True
                    preprocesssed_current_company_name = preprocess_name(str(the_comp_name))
                    if preprocesssed_current_company_name in title.lower().replace(" ", ""):
                        search_name_in_title = True
                    if preprocesssed_current_company_name in domain_url.lower().replace(" ", ""):
                        search_name_in_url  = True

                    if search_name_in_url or search_name_in_title or has_current_address or has_addresses or abbraviation_name_url_match or name_domain_ful_match:
                        found_web_page = True
                        inserRowDb(mycol, entry["current_name"], entry["names"], domain_url, isAlive, entry["current_address"],
                                   entry["addresses"], entry["corp_id"], title, description, main, about, products, keywords,
                                   contact, img, has_current_address,has_addresses,service, directory, search_name_in_url,
                                   search_name_in_title, entry["business_n0"],abbraviation_name_url_match,name_domain_ful_match,logo,the_comp_name)
                        break
            except Exception as e:
                skipped_companies_due_to_error.append(entry["corp_id"])
                with open('./0_to_50000_skipped_companies.pkl', 'wb') as file:
                    pickle.dump(skipped_companies_due_to_error, file)
                print(e)
                continue
    if  found_web_page == False:
        if INSERT_NO_FOUND_WEBPAGE_COMPANY:
            inserRowDb(mycol, entry["current_name"], entry["names"], "", False, entry["current_address"],
                       entry["addresses"], entry["corp_id"], "", "", "", "", "", "",
                       "", "", False, False, "", "", False,
                       False, entry["business_n0"],False,False,"","")
    count += 1
    with open('0_to_50000.pkl', 'wb') as file:
        pickle.dump(count, file)
\end{mintedbox}

\subsection{Setting up code for scrapping program}
Scrapping code setup instructions for (Windows):
The code to scrap the web page can be found in the GitHub repository under scrapping folder. The first step is to clone the repository to your local machine.  

We will use the MongoDB database to store the content of the scrapped web pages. The first step is to set up the MongoDB database on the local computer. The MongoDB installer needs to be downloaded \hyperlink{https://fastdl.mongodb.org/windows/mongodb-windows-x86_64-5.0.2-signed.msi}{\textcolor{green}{here}}.

Click the provided link to download the MongoDB installer and then launch the $MongoDB-windows-x86_64-5.0.0-signed.msi$ MongoDB installer by double click on it. We should see: figure~\ref{fig:mongo_installer}.

\begin{figure}[h!]
  \includegraphics[width=10cm]{mongo_installer.PNG}
  \caption{The MongoDB installer interface}
  \label{fig:mongo_installer}
\end{figure}

Just click through and leave every setting as they are. Once the installation for the MongoDB is complete， launch the MongoDB database once by clicking on the file named mongo in the folder “MongoDB/Server/5.0/bin” as shown in the following picture. The MongoDB folder is usually in the Program File folder in the C:/ driver unless we set a specific installation path during the installation of the MongoDB. 

Now that MongoDB is setup. We will install MongoDB compass to have a graphic user interface to better visualize and manipulate the database content. So download the zipped folder for MongoDB compass \href{https://downloads.mongodb.com/compass/mongodb-compass-1.28.4-win32-x64.zip}{here}:  , unzip the zipped folder and go inside the resulting unzipped folder and launch the MongoDB compass by double click on it as shown in ~\ref{fig:mongo_compass}

\begin{figure}[h!]
  \includegraphics[width=13cm]{mongo_compass.PNG}
  \caption{The MongoDB Compass}
  \label{fig:mongo_compass}
\end{figure}

The MongoDB compass will be launched and after clicking through the introduction messages, We will arrive at an interface as shown ~\ref{fig:mongo_compass_start}

\begin{figure}[h!]
  \includegraphics[width=13cm]{mongo_compass_start.PNG}
  \caption{The MongoDB Compass interface}
  \label{fig:mongo_compass_start}
\end{figure}

Click on the New Connection tab on the left panel of the GUI, then select Fill in connection fields individually. Then leave everything as default and then connect. Once connected, there will be a local database in which We can create a new collection as shown in the image:~\ref{fig:mongo_create_db}
\begin{figure}[h!]
  \includegraphics[width=13cm]{mongo_create_db.PNG}
  \caption{Creating a database using MongoDB}
  \label{fig:mongo_create_db}
\end{figure}

Create the new collection and name the collection $federal\_corporation\_scrapping$ then download the federal corporation list from the statistics of Canada website at this \href{https://open.canada.ca/data/dataset/0032ce54-c5dd-4b66-99a0-320a7b5e99f2}{\textcolor{green}{url}}  and put the unzipped folder called OPEN\_DATA\_SPLIT in the scrapping folder as shown in figure ~\ref{fig:open_data_split}
\begin{figure}[h!]
  \includegraphics[width=13cm]{open_data_split.PNG}
  \caption{The Open Data Split folder in Pycharm}
  \label{fig:open_data_split}
\end{figure}

The downloaded company list from the statistics Canada website is in the XML format, and we will now set up a database to store the list of companies with the information about them that is important to us. So go ahead and create a collection called comp\_list in our MongoDB as shown in figure ~\ref{fig:comp_list}
\begin{figure}[h!]
  \includegraphics[width=13cm]{comp_list_mongodb.PNG}
  \caption{Created Company list collection in MongoDB}
  \label{fig:comp_list}
\end{figure}


Now, run the get\_federal\_corp\_data\_list.py scrip to fill the database. Once done, our database would look something like that: ~\ref{fig:fed_corp_list_mongo}
\begin{figure}[h!]
  \includegraphics[width=13cm]{mongo_fed_corp_list.PNG}
  \caption{The federal incorporated companies in MongoDB}
  \label{fig:fed_corp_list_mongo}
\end{figure}

\textcolor{red}{So we now have a list of federally incorporated companies without their web page information in the database.  So we are finally ready to run the scrapping program. First, we can export this company list from the database as JSON file. The way to do it is to go to the collection tab of MongoDB and select export collection as shown in} figure ~\ref{fig:export_collection}.
\begin{figure}[h!]
  \includegraphics[width=13cm]{export_collection.PNG}
  \caption{How to export data from MongoDB compass}
  \label{fig:export_collection}
\end{figure}

\textcolor{red}{}An alternative method is to download the already made list from my Google drive at the \href{https://drive.google.com/file/d/14TlsPSoVK4fIQ1q4jg_HzydgFvKP9v3_/view?usp=sharing}{URL}. 
Now put this file in the same folder as the scrapping program as shown in figure ~\ref{fig:fed_scrap_image}
\begin{figure}[h!]
  \includegraphics[width=13cm]{fed_scrap_image.PNG}
  \caption{Placement of the scrapping files in Pycharm}
  \label{fig:fed_scrap_image}
\end{figure}

and run the scrapping program to start the scrapping!

\subsection{Documentation for scrapped data}

The scrapped data has been uploaded to the MongoDB server; as shown in figure ~\ref{fig:classification_file}
\begin{figure}[h!]
  \includegraphics[width=13cm]{classification_files.PNG}
  \caption{The uploaded classification files}
  \label{fig:classification_file}
\end{figure}
, we briefly describe each collection now. 

\begin{itemize}
\item 60kfed\_companies is one of the two final version of the scrapped federally incorporated company web pages. 

\item federal\_corporation\_data\_final\_version is the other final version of the scrapped federally incorporated company web pages. 

    \item a\_new\_collection is some of the scrapped federally incorporated companies. Some of the samples don't have an URL. So this collection could be redundant.
    
    \item alive\_corporations are some federally incorporated corporations that are active. However, no URL is provided, so this collection could be redundant. 
    
    \item c0-5000\_v2 collection is the 0 to 5000 companies of scrapped data using the Clearbit API service. The raw HTML content of the home page of the found web page of the company is stored.
    
    \item 
c5000-10000 collection is the 5000 to 10000 companies of scrapped data using the Clearbit API service. The raw HTML content of the home page of the company's web page is stored.
    
    \item 
collection0 collection is about 250,000 companies from Clearbit API. The content of about, contact, main, product pages is scrapped. However, we believe that the scrapped information is incomplete. A more sophisticated scrapping program should be used to scrap the web pages' information. 

\item compagny\_url\_classify is not good data and can be removed. 

\item correct\_url\_col is supposed to be a company with a correct URL that was used to train a classification model that can tell the right web page for a company from the incorrect ones. However, the company's correctness of the web page is not 100 percent accurate. 
\end{itemize}

The rest of the data are randomly scrapped from either SICCODE.com or federally incorporated companies or Clearbit data. 



\subsection{Setting up code for classification program}
We have three classification files uploaded on Github, as shown in figure ~\ref{fig:mongocollection}
\begin{figure}[h!]
  \includegraphics[width=13cm]{MongoDBServerCollections.PNG}
  \caption{The MongoDB server collections}
  \label{fig:mongocollection}
\end{figure}

, and each of them has a similar setup. We'll explain how to run the tree models one by one. But first, we need to download all files in a folder. 
\begin{itemize}
    \item To set up the industry\_classify\_keyword\_method.py file, we just need to make sure that we point to the right testing collection in the MongoDB database. So we need potentially change the MongoDB set up in lines 41-43 of the code, and then we need to run the program. We are using a set of keywords generated using the TF-IDF method from the NAICS definition and examples. We have given a threshold to select the keywords with only a TF-IDF value above that threshold. If we want to use a different threshold, we can edit the threshold value in the Create\_industry\_vocabulary.py file at line 15. Then we run the Create\_industry\_vocabulary.py to print out the set of keywords. Then we can copy and paste the keywords in the industry\_classify\_keyword\_method.py file at lines 284-311. The correct collection to set is the official\_test collection, as shown in figure 8. 
    
    \item For the traditional\_classification\_methods.py file, we need to import the training collection as a JSON file and put that file in the same folder as the program file. Then we need to set up the MongoDB pointer as before in lines 27-29. The training data collection is called official\_train in the MongoDB server. 
    
    \item For the transformer_based_classification.ipynb file, we can run the file in the Colab notebook. We just need to make sure that we put the correct path for the training data in the program. 
\end{itemize}




\bibliographystyle{alpha}
\bibliography{references} % see references.bib for bibliography management
\end{flushleft}
\end{document}
